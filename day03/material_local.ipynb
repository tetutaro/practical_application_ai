{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKZ1-5i-zKe0"
   },
   "source": [
    "# 実践的データ分析講座 ３日目：ハンズオン\n",
    "\n",
    "* １日目の座学でざっと説明した分析を、データを使って実際に行っていきます\n",
    "* データは [Kaggle](https://www.kaggle.com/) に収録されている [Medical Cost Personal Datasets](https://www.kaggle.com/mirichoi0218/insurance) を使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVQh01ObHmYk"
   },
   "outputs": [],
   "source": [
    "# おまじない\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVdAMtfMNTp6"
   },
   "source": [
    "# 前処理\n",
    "\n",
    "最も重要かつ大変だけど、地味で辛い作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGxVGvrDNXwR"
   },
   "source": [
    "## データの読み込み\n",
    "\n",
    "* CSV データを読み込む\n",
    "    * 先頭行にカラムの名前が付いた、ヘッダ付き CSV\n",
    "    * pandas パッケージを用いて、CSV を読み込むと共に内容を DataFrame 形式に変換し格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRHKVY9DNjxr"
   },
   "outputs": [],
   "source": [
    "# CSVファイルの読み込み\n",
    "data = pd.read_csv('insurance.csv', header=0)\n",
    "# 先頭5行を表示\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NfCxonSOy9L"
   },
   "source": [
    "## データの説明\n",
    "\n",
    "ある国の住人が、医療費としていくら支払ったかのデータ\n",
    "\n",
    "* `age`: 年齢\n",
    "* `sex`: 性別\n",
    "* `bmi`: ボディマス指数（Body Mass Index: 体重(kg)÷(身長(m))${}^2$）\n",
    "* `children`: 養育している子供の数\n",
    "* `smoker`: 喫煙しているか否か\n",
    "* `region`: 住んでいる地域\n",
    "* `charges`: 支払った医療費の額"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK577EN9O2vQ"
   },
   "outputs": [],
   "source": [
    "# データの数を調べる\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq9l1zJCO5oV"
   },
   "source": [
    "## 欠損値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcgGirVhO9L9"
   },
   "outputs": [],
   "source": [
    "# 欠損しているデータ数を調べる\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkA-rcORO_lS"
   },
   "outputs": [],
   "source": [
    "# データ除去前の個数確認\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKV3MAnTPCFK"
   },
   "outputs": [],
   "source": [
    "# データが欠損している行（データ点）を除去する\n",
    "data.dropna(axis='index', how='any', inplace=True)\n",
    "# データ除去後の個数確認\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6ypfLQ-PE8M"
   },
   "source": [
    "## データ重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9ke28c2PHXq"
   },
   "outputs": [],
   "source": [
    "# データが重複している数を調べる\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAeTdfLtPJ3t"
   },
   "outputs": [],
   "source": [
    "# データ除去前の個数確認\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgebYtriPMw6"
   },
   "outputs": [],
   "source": [
    "# 重複している行（データ点）を除去する\n",
    "data.drop_duplicates(keep='first', ignore_index=True, inplace=True)\n",
    "# データ除去後の個数確認\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QK2z30wPPY8"
   },
   "source": [
    "## 分布\n",
    "\n",
    "データを見る限り、それぞれの項目は以下のデータ種別でありそう。\n",
    "\n",
    "* `age`: 整数の連続変数\n",
    "* `sex`: 文字列のカテゴリ変数\n",
    "* `bmi`: 実数の連続変数\n",
    "* `children`: 整数の連続変数\n",
    "* `smoker`: 文字列の\n",
    "* `region`: 文字列のカテゴリ変数\n",
    "* `charges`: 実数の連続変数\n",
    "\n",
    "これらがどのような分布を示すか調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5aBJUSRPSaT"
   },
   "outputs": [],
   "source": [
    "# `age` の分布\n",
    "data['age'].plot(kind='hist', bins=int(np.sqrt(data.shape[0])))\n",
    "plt.xlabel('age')\n",
    "plt.title('distribution of age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obaHWYsgPVVf"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# `sex` の分布\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data['sex'])).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnzaIjlYPX0Q"
   },
   "outputs": [],
   "source": [
    "# `bmi` の分布\n",
    "data['bmi'].plot(kind='hist', bins=int(np.sqrt(data.shape[0])))\n",
    "plt.xlabel('bmi')\n",
    "plt.title('distribution of BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVgJDGHJPZ5S"
   },
   "outputs": [],
   "source": [
    "# `children` の分布（カテゴリ変数として）\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data['children'])).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUtpRljYPcHG"
   },
   "outputs": [],
   "source": [
    "# `children` の分布（連続変数として）\n",
    "data['children'].plot(kind='hist', bins=int(max(data['children'] + 1)))\n",
    "plt.xlabel('children')\n",
    "plt.title('distribution of children')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMxYzknxPegY"
   },
   "outputs": [],
   "source": [
    "# `smoker` の分布\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data['smoker'])).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4eNNj-tPgnf"
   },
   "outputs": [],
   "source": [
    "# `region` の分布\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data['region'])).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsfELU7XPnv5"
   },
   "outputs": [],
   "source": [
    "# `charges` の分布\n",
    "data['charges'].plot(kind='hist', bins=int(np.sqrt(data.shape[0])))\n",
    "plt.xlabel('charges')\n",
    "plt.title('distribution of charges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h-JmbPtPqpN"
   },
   "source": [
    "## カテゴリ変数のダミー変数化\n",
    "\n",
    "* 対象となるカラム（カテゴリ変数）\n",
    "    * `sex`\n",
    "    * `smoker`\n",
    "    * `region`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILUZ3pi-PtRu"
   },
   "outputs": [],
   "source": [
    "# ダミー変数化の説明として、`region` を単純にダミー変数化し、比較する\n",
    "pd.merge(\n",
    "    data.loc[:, ['region']],\n",
    "    pd.get_dummies(data['region'], prefix='region_is'),\n",
    "    left_index=True, right_index=True\n",
    ").head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDM-RN9iPv5Y"
   },
   "outputs": [],
   "source": [
    "# このままだとマルチコを引き起こすので、適当にひとつ除去する\n",
    "pd.merge(\n",
    "    data.loc[:, ['region']],\n",
    "    pd.get_dummies(data['region'], prefix='region_is', drop_first=True),\n",
    "    left_index=True, right_index=True\n",
    ").head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXyWepggPy_F"
   },
   "outputs": [],
   "source": [
    "# すべてのカテゴリ変数をダミー変数化し、元のデータにくっつける\n",
    "data = pd.concat([\n",
    "    data,\n",
    "    pd.get_dummies(data['sex'], prefix='sex_is', drop_first=True),\n",
    "    pd.get_dummies(data['smoker'], prefix='smoker_is', drop_first=True),\n",
    "    pd.get_dummies(data['region'], prefix='region_is', drop_first=True),\n",
    "], axis='columns')\n",
    "# 元のカテゴリ変数を除去\n",
    "data.drop(['sex', 'smoker', 'region'], axis='columns', inplace=True)\n",
    "# 先頭５行を表示\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqpUePz_P2_4"
   },
   "outputs": [],
   "source": [
    "# データの個数確認\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjQL2RbbP6o6"
   },
   "source": [
    "## データの統計値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k2Aj05mP9OF"
   },
   "outputs": [],
   "source": [
    "### describe() でデータの統計値を簡単に確認できる\n",
    "data.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nCg2uOiQADj"
   },
   "source": [
    "## 異常値\n",
    "\n",
    "* 異常値かどうかは、その値から単純に判定できない\n",
    "* 分布やデータの意味を理解した上で判定しなければならない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoVGHblrQCqd"
   },
   "outputs": [],
   "source": [
    "# すべてのカラムの箱ひげ図を並べて表示するため、簡易的に正規化する\n",
    "data_normed = (data - data.mean(axis='index')) / data.std(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7_mgYsZdOzZ"
   },
   "outputs": [],
   "source": [
    "# 箱ひげ図を表示する\n",
    "data_normed.plot(kind='box')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('box plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHVCSCmwdVqu"
   },
   "source": [
    "異常値をどのように判断するか\n",
    "\n",
    "* ダミー変数は（箱ひげ図で外れ値となっても）異常値として扱わない\n",
    "    * データが不均衡（imbalance）の場合は、簡単に異常値と判定されてしまう\n",
    "* `bmi` の外れ値\n",
    "    * BMIが大きい人（極度の肥満）は、健康に問題を抱えている可能性が高い\n",
    "    * ゆえに医療費が高額になると予想される\n",
    "    * 箱ひげ図としては外れ値だが、重要な意味を持つデータである\n",
    "    * よって、異常値としては扱わない（除外しない）\n",
    "* `children` の外れ値\n",
    "    * 扶養する子供が多い人は、医療費を多く支払う可能性がある\n",
    "    * 指数分布のような分布をしており、値が多い場合でも異常値とは見做せない\n",
    "    * よって、異常値としては扱わない（除外しない）\n",
    "* `charges` の外れ値\n",
    "    * 医療費が極端に高額な人は、何か問題を抱えていると考えられ、重要なデータである\n",
    "    * べき分布のような分布をしており、値が多い場合でも異常値とは見做せない\n",
    "    * よって、異常値としては扱わない（除外しない）\n",
    "* 以上から、今回のデータでは、異常値の除外処理は行わない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32-Oe4C8dad6"
   },
   "source": [
    "## 相関"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln27fMBDdd5O"
   },
   "outputs": [],
   "source": [
    "# すべてのカラムの分布と相関を一度に見る簡易的な方法\n",
    "# カラム数が多い場合はお薦めできない\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvJaXtnNdf-Z"
   },
   "outputs": [],
   "source": [
    "# 相関係数の絶対値をヒートマップで表示する\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.heatmap(data.corr().abs(), cmap='jet', annot=True, fmt='0.2f', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NJ28rWvdks3"
   },
   "source": [
    "# モデル作成と評価（分類）\n",
    "\n",
    "* 目的変数として `smoker_is_yes` を設定する\n",
    "    * 各種データから、その人が喫煙者かどうかを予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uM8kaRrdoJP"
   },
   "source": [
    "## 目的変数・特徴量の分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MP6jn-1dqws"
   },
   "outputs": [],
   "source": [
    "# 目的変数・特徴量の分離\n",
    "data_classify_y = data['smoker_is_yes']\n",
    "data_classify_X = data.drop('smoker_is_yes', axis='columns')\n",
    "# データ数の確認\n",
    "data_classify_X.shape, data_classify_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt5j8JLFdtj8"
   },
   "source": [
    "## 学習用・検証用データの分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lzqtqh7QdwGS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 学習用・検証用データの分離\n",
    "# `smoker_is_yes` は imbalance なので、stratify を指定する\n",
    "(\n",
    "    data_classify_train_X, data_classify_test_X,\n",
    "    data_classify_train_y, data_classify_test_y\n",
    ") = train_test_split(\n",
    "    data_classify_X, data_classify_y,\n",
    "    stratify=data_classify_y,\n",
    "    test_size=0.3, random_state=14\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27um45WFd02r"
   },
   "source": [
    "## 重要なポイント：再現性の確保\n",
    "\n",
    "* scikit-learn パッケージ(sklearn)で `random_state` バラメータがある関数は、必ずこれを指定する\n",
    "    * 乱数を用いるアルゴリズム\n",
    "    * `random_state` を付けないと、関数を実行するたびに違う結果になる\n",
    "    * 良い精度のモデルが作成できたとしても、それを再現することが出来ない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLG73VwQd4Io"
   },
   "outputs": [],
   "source": [
    "# データ数の確認（学習用データ）\n",
    "data_classify_train_X.shape, data_classify_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBjDp9fpd6Sc"
   },
   "outputs": [],
   "source": [
    "# データ数の確認（検証用データ）\n",
    "data_classify_test_X.shape, data_classify_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6ysQBaud8aI"
   },
   "outputs": [],
   "source": [
    "# 目的変数の分布の確認（学習用データ）\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data_classify_train_y)).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDX4ZCQsd-ev"
   },
   "outputs": [],
   "source": [
    "# データ数に対する喫煙者の割合（学習用データ）\n",
    "round(\n",
    "    Counter(data_classify_train_y)[1] / data_classify_train_y.shape[0], 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrS1snAseBID"
   },
   "outputs": [],
   "source": [
    "# 目的変数の分布の確認（検証用データ）\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data_classify_test_y)).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYPSI05SeDqA"
   },
   "outputs": [],
   "source": [
    "# データ数に対する喫煙者の割合（検証用データ）\n",
    "round(\n",
    "    Counter(data_classify_test_y)[1] / data_classify_test_y.shape[0], 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq43pujdeGiX"
   },
   "source": [
    "## 特徴量エンジニアリング\n",
    "\n",
    "* 残念ながら、このデータへの知見は持ち合わせてないので、知見による特徴エンジニアリングは出来ない\n",
    "* 対数化\n",
    "    * `charges` の分布がべき分布に近いので、`charges` を対数化する\n",
    "* 離散化\n",
    "    * `charges` の分布には３つの山があるように見えるので、`charges` を３つにクラスタリングする\n",
    "* 本来ならば、同じカラムに２種類の特徴量エンジニアリングはやらない\n",
    "    * マルチコになるだけだから\n",
    "    * 今回は例として無理に行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uta64stYeJWe"
   },
   "outputs": [],
   "source": [
    "# `charges` の対数化\n",
    "data_classify_train_X = data_classify_train_X.assign(\n",
    "    log_charges=np.log(data_classify_train_X['charges'] + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJZbZjVQeRG-"
   },
   "outputs": [],
   "source": [
    "# クラスタリングとしては k-Means を用いる（1次元 k-Means）\n",
    "# k-Means は距離を用いるため、正規化が必要\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 正規化\n",
    "ss_kmeans = StandardScaler()\n",
    "data_clustering_train_X = ss_kmeans.fit_transform(\n",
    "    data_classify_train_X.loc[:, ['charges']]\n",
    ")\n",
    "# k-Meansの学習\n",
    "km_charges = KMeans(\n",
    "    n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=14\n",
    ").fit(\n",
    "    data_clustering_train_X\n",
    ")\n",
    "# クラスタ番号は適当に付くので、中心の値が小さい順に振り直す\n",
    "# そのための辞書を作成\n",
    "km_cluster_map = {\n",
    "    c: f'cluster{i}' for i, c in enumerate(\n",
    "        km_charges.cluster_centers_.ravel().argsort()\n",
    "    )\n",
    "}\n",
    "# k-Meansの予測結果を振り直しつつ格納\n",
    "data_classify_train_X = data_classify_train_X.assign(\n",
    "    charge_cluster=[\n",
    "        km_cluster_map[x] for x in km_charges.predict(data_clustering_train_X)\n",
    "    ]\n",
    ")\n",
    "# k-Meansの結果を可視化\n",
    "sns.displot(\n",
    "    data=data_classify_train_X, x='charges', palette='tab10',\n",
    "    hue='charge_cluster', hue_order=['cluster0', 'cluster1', 'cluster2']\n",
    ")\n",
    "plt.title('clustering (discretize) charges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HYRSHrFeWLi"
   },
   "outputs": [],
   "source": [
    "# k-Meansの予測結果をダミー変数化\n",
    "data_classify_train_X = pd.concat([\n",
    "    data_classify_train_X,\n",
    "    pd.get_dummies(\n",
    "        data_classify_train_X['charge_cluster'],\n",
    "        prefix='charge_is', drop_first=True\n",
    "    ),\n",
    "], axis='columns')\n",
    "# `charges` と `charge_cluster` を除去\n",
    "data_classify_train_X.drop(\n",
    "    ['charges', 'charge_cluster'], axis='columns', inplace=True\n",
    ")\n",
    "# 先頭５行を表示\n",
    "data_classify_train_X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftivhh6eeYw3"
   },
   "outputs": [],
   "source": [
    "# 検証用データにも同じことをする\n",
    "# （学習は一切行わず、予測のみ）\n",
    "\n",
    "# 対数化\n",
    "data_classify_test_X = data_classify_test_X.assign(\n",
    "    log_charges=np.log(data_classify_test_X['charges'] + 1)\n",
    ")\n",
    "# k-Meansのための正規化\n",
    "data_clustering_test_X = ss_kmeans.transform(\n",
    "    data_classify_test_X.loc[:, ['charges']]\n",
    ")\n",
    "# k-Meansの予測結果を格納\n",
    "data_classify_test_X = data_classify_test_X.assign(\n",
    "    charge_cluster=[\n",
    "        km_cluster_map[x] for x in km_charges.predict(data_clustering_test_X)\n",
    "    ]\n",
    ")\n",
    "# k-Meansの予測結果をダミー変数化\n",
    "data_classify_test_X = pd.concat([\n",
    "    data_classify_test_X,\n",
    "    pd.get_dummies(\n",
    "        data_classify_test_X['charge_cluster'],\n",
    "        prefix='charge_is', drop_first=True\n",
    "    ),\n",
    "], axis='columns')\n",
    "# `charges` と `charge_cluster` を除去\n",
    "data_classify_test_X.drop(\n",
    "    ['charges', 'charge_cluster'], axis='columns', inplace=True\n",
    ")\n",
    "# 先頭５行を表示\n",
    "data_classify_test_X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgLRf3Q3eh28"
   },
   "outputs": [],
   "source": [
    "# データ数の確認\n",
    "data_classify_train_X.shape, data_classify_test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5o_wxvCekHg"
   },
   "outputs": [],
   "source": [
    "# 特徴量を追加したため、相関を再確認する\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    data_classify_train_X.corr().abs(),\n",
    "    cmap='jet', annot=True, fmt='0.2f', ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_IQDTNoeyZE"
   },
   "source": [
    "## 正規化\n",
    "\n",
    "* Support Vector Machine のために、データを正規化する\n",
    "    * Random Forest は正規化したデータは使わない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwhNCng8e10m"
   },
   "outputs": [],
   "source": [
    "# 正規化\n",
    "ss_classify = StandardScaler().fit(\n",
    "    data_classify_train_X\n",
    ")\n",
    "# 学習用データ\n",
    "data_classify_train_X_normed = ss_classify.transform(\n",
    "    data_classify_train_X\n",
    ")\n",
    "# 検証用データ\n",
    "data_classify_test_X_normed = ss_classify.transform(\n",
    "    data_classify_test_X\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7irwrLhWs-4e"
   },
   "source": [
    "## 次元圧縮\n",
    "\n",
    "今回は次元圧縮したデータは使わないが、例として行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRVch-t6e-ZD"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 圧縮しても十分に意味が残る累積寄与率の閾値を 0.8 とする\n",
    "cumsum_variance_ratio_threshold = 0.8\n",
    "# まずは全く圧縮しないPCAを行う\n",
    "pca_full = PCA(\n",
    "    n_components=data_classify_train_X_normed.shape[1], random_state=14\n",
    ").fit(data_classify_train_X_normed)\n",
    "# 累積寄与率のグラフを描く\n",
    "pd.DataFrame({\n",
    "    'dimension': range(1, data_classify_train_X_normed.shape[1] + 1),\n",
    "    'cumsum_variance_ratio': np.cumsum(pca_full.explained_variance_ratio_),\n",
    "}).plot(kind='line', x='dimension', y='cumsum_variance_ratio')\n",
    "plt.ylabel('cumsum variance ratio')\n",
    "plt.title('dimension vs cumsum variance ratio')\n",
    "plt.axhline(cumsum_variance_ratio_threshold, color='red')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_oL4AsfBNc"
   },
   "outputs": [],
   "source": [
    "# 圧縮後の次元数の計算\n",
    "decomposed_dimension = range(\n",
    "    1, data_classify_train_X_normed.shape[1] + 1\n",
    ")[\n",
    "    np.min(np.where(np.cumsum(\n",
    "        pca_full.explained_variance_ratio_\n",
    "    ) > cumsum_variance_ratio_threshold))\n",
    "]\n",
    "decomposed_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWvCFTEffEVD"
   },
   "outputs": [],
   "source": [
    "# 算出した次元数へ圧縮\n",
    "pca_decompose = PCA(\n",
    "    n_components=decomposed_dimension, random_state=14\n",
    ").fit(data_classify_train_X_normed)\n",
    "# 学習データを圧縮\n",
    "data_classify_train_X_decomposed = pca_decompose.transform(\n",
    "    data_classify_train_X_normed\n",
    ")\n",
    "# 検証データを圧縮\n",
    "data_classify_test_X_decomposed = pca_decompose.transform(\n",
    "    data_classify_test_X_normed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C1t9LvRIPM9"
   },
   "outputs": [],
   "source": [
    "data_classify_train_X_decomposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-oD9kzp_wyp"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data_classify_train_X_decomposed).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poBtnh_LfLpj"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukpkf_4fPPd"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "# 目的変数がimbalanceの場合は、`class_weight` として None と 'balanced' を探索する\n",
    "hparams_classify_svc = {\n",
    "    'C': expon(scale=1),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "# Cross Validation\n",
    "# マルチコが発生する可能性があるため、ペナルティをL2に設定\n",
    "# Cross Validation の評価指標としてはF1値を採用\n",
    "# 特徴量は、特徴量エンジニアリングを行い、正規化したものを用いる\n",
    "svc_classify = LinearSVC(penalty='l2', max_iter=1000, random_state=14)\n",
    "skf_classify_svc = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_classify_svc = RandomizedSearchCV(\n",
    "    svc_classify, hparams_classify_svc, cv=skf_classify_svc, scoring='f1',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_classify_train_X_normed, data_classify_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLujwRG6fUtI"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_classify_svc.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUTXIfBMnsAr"
   },
   "outputs": [],
   "source": [
    "# 学習データで作成した最善モデルの、予測結果のF1値\n",
    "hpsearch_classify_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgNV3ZpV7to2"
   },
   "outputs": [],
   "source": [
    "# 偏回帰係数の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': ['(intercept)'] + list(data_classify_train_X.columns),\n",
    "    'coef': list(\n",
    "        hpsearch_classify_svc.best_estimator_.intercept_\n",
    "    ) + list(\n",
    "        hpsearch_classify_svc.best_estimator_.coef_[0]\n",
    "    ),\n",
    "}).plot(kind='barh', x='feature', y='coef')\n",
    "plt.xlabel('value of coefficient')\n",
    "plt.title('coefficient')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZWH9glxnubY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 検証データで予測する\n",
    "pred_classify_svc_test = hpsearch_classify_svc.best_estimator_.predict(\n",
    "    data_classify_test_X_normed\n",
    ")\n",
    "# 検証データの予測結果を評価する\n",
    "pd.DataFrame(classification_report(\n",
    "    data_classify_test_y, pred_classify_svc_test, output_dict=True,\n",
    "    target_names=['smorker_is_no', 'smorker_is_yes']\n",
    ")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN5v1jB3nykY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 検証データの正解・不正解を可視化する\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(data_classify_test_y, pred_classify_svc_test).T,\n",
    "    index=['predicted_smorker_is_no', 'predicted_smorker_is_yes'],\n",
    "    columns=['smorker_is_no', 'smorker_is_yes']\n",
    ").loc[\n",
    "    ['predicted_smorker_is_yes', 'predicted_smorker_is_no'],\n",
    "    ['smorker_is_yes', 'smorker_is_no']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq3e1OOTn0zk"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbPffH-vn3QE"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "# 目的変数がimbalanceの場合は、`class_weight` として None と 'balanced' を探索する\n",
    "hparams_classify_rfc = {\n",
    "    'n_estimators': randint(low=10, high=100),\n",
    "    'max_depth': randint(low=2, high=10),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "# Cross Validation\n",
    "# マルチコが発生する可能性があるため、ペナルティをL2に設定\n",
    "# Cross Validation の評価指標としてはF1値を採用\n",
    "# 特徴量は、特徴量エンジニアリングは行ったが、正規化していないものを用いる\n",
    "rfc_classify = RandomForestClassifier(random_state=14)\n",
    "skf_classify_rfc = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_classify_rfc = RandomizedSearchCV(\n",
    "    rfc_classify, hparams_classify_rfc, cv=skf_classify_rfc, scoring='f1',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_classify_train_X, data_classify_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLjMROE2n6so"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_classify_rfc.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P5DEYOVn_P6"
   },
   "outputs": [],
   "source": [
    "# 学習データで作成した最善モデルの、予測結果のF1値\n",
    "hpsearch_classify_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOSAJbW9oAC_"
   },
   "outputs": [],
   "source": [
    "# feature importance（特徴量の重要度）の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': data_classify_train_X.columns,\n",
    "    'importance': hpsearch_classify_rfc.best_estimator_.feature_importances_,\n",
    "}).plot(kind='barh', x='feature', y='importance')\n",
    "plt.xlabel('importance')\n",
    "plt.title('importance of features')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC-UhSApoCp2"
   },
   "outputs": [],
   "source": [
    "# 検証データで予測する\n",
    "pred_classify_rfc_test = hpsearch_classify_rfc.best_estimator_.predict(\n",
    "    data_classify_test_X\n",
    ")\n",
    "# 検証データの予測結果を評価する\n",
    "pd.DataFrame(classification_report(\n",
    "    data_classify_test_y, pred_classify_rfc_test, output_dict=True,\n",
    "    target_names=['smorker_is_no', 'smorker_is_yes']\n",
    ")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDODauOKoFG3"
   },
   "outputs": [],
   "source": [
    "# 検証データの正解・不正解を可視化する\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(data_classify_test_y, pred_classify_rfc_test).T,\n",
    "    index=['predicted_smorker_is_no', 'predicted_smorker_is_yes'],\n",
    "    columns=['smorker_is_no', 'smorker_is_yes']\n",
    ").loc[\n",
    "    ['predicted_smorker_is_yes', 'predicted_smorker_is_no'],\n",
    "    ['smorker_is_yes', 'smorker_is_no']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFsDF25qoH9H"
   },
   "source": [
    "# モデル作成と評価（回帰）\n",
    "\n",
    "* 目的変数として `charges` を設定する\n",
    "    * 各種データから、その人の医療費を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U-kQR8OoNtw"
   },
   "source": [
    "## 目的変数・特徴量の分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYaobcH-oP0Q"
   },
   "outputs": [],
   "source": [
    "# 目的変数・特徴量の分離\n",
    "data_regress_y = data['charges']\n",
    "data_regress_X = data.drop('charges', axis='columns')\n",
    "# データ数の確認\n",
    "data_regress_X.shape, data_regress_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiyU0RZ_oTrM"
   },
   "source": [
    "## 学習用・検証用データの分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRqjOyS4oWGY"
   },
   "outputs": [],
   "source": [
    "# 学習用・検証用データの分離\n",
    "# 回帰なのでimbalanceという概念は存在せず、stratifyは出来ない\n",
    "(\n",
    "    data_regress_train_X, data_regress_test_X,\n",
    "    data_regress_train_y, data_regress_test_y\n",
    ") = train_test_split(\n",
    "    data_regress_X, data_regress_y,\n",
    "    test_size=0.3, random_state=14\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkDKq0gZoaVi"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RoZ4srPoc9c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "hparams_regress_svr = {\n",
    "    'C': expon(scale=1),\n",
    "}\n",
    "# Cross Validation\n",
    "# Cross Validation の評価指標としては、`charges` がべき分布なので、MSLE（平均２乗対数誤差）を採用\n",
    "# 線形 Shallow モデルで回帰を行うが、目的変数に大きな値が含まれる場合、\n",
    "# 偏回帰係数がペナルティで抑え込まれてしまうため、非常に悪い結果になる。\n",
    "# よって、今回は特徴量の正規化をわざと行わない。\n",
    "svr_regress = LinearSVR(max_iter=1000, random_state=14)\n",
    "kf_regress_svr = KFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_regress_svr = RandomizedSearchCV(\n",
    "    svr_regress, hparams_regress_svr, cv=kf_regress_svr,\n",
    "    scoring='neg_mean_squared_log_error',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_regress_train_X, data_regress_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st5To8-Bo6Jo"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_regress_svr.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyavc0MJo8uK"
   },
   "outputs": [],
   "source": [
    "# 偏回帰係数の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': ['(intercept)'] + list(data_regress_train_X.columns),\n",
    "    'coef': list(\n",
    "        hpsearch_regress_svr.best_estimator_.intercept_\n",
    "    ) + list(\n",
    "        hpsearch_regress_svr.best_estimator_.coef_\n",
    "    ),\n",
    "}).plot(kind='barh', x='feature', y='coef')\n",
    "plt.xlabel('coefficient')\n",
    "plt.title('value of coefficient')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSpSFrPzo_7a"
   },
   "outputs": [],
   "source": [
    "# 検証データで予測する\n",
    "pred_regress_svr_test = hpsearch_regress_svr.best_estimator_.predict(\n",
    "    data_regress_test_X\n",
    ")\n",
    "# 検証データの予測結果を可視化する\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "pd.DataFrame({\n",
    "    'true': data_regress_test_y,\n",
    "    'predicted': pred_regress_svr_test\n",
    "}).plot(kind='scatter', x='true', y='predicted', ax=ax, color=sns.color_palette('tab10')[0])\n",
    "maxval = max(np.max(data_regress_test_y), np.max(pred_regress_svr_test))\n",
    "ax.plot((0, maxval), (0, maxval), color='red')\n",
    "plt.xlim((0, 1.03 * maxval))\n",
    "plt.ylim((0, 1.03 * maxval))\n",
    "plt.title('prediction of `charges` (Support Vector Machine)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2UVUFyPpEaY"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQTMBwgxpH_Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "hparams_regress_rfr = {\n",
    "    'n_estimators': randint(low=10, high=200),\n",
    "    'max_depth': randint(low=2, high=10),\n",
    "}\n",
    "# Cross Validation\n",
    "# Cross Validation の評価指標としては、`charges` がべき分布なので、MSLE（平均２乗対数誤差）を採用\n",
    "rfr_regress = RandomForestRegressor(random_state=14)\n",
    "kf_regress_rfr = KFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_regress_rfr = RandomizedSearchCV(\n",
    "    rfr_regress, hparams_regress_rfr, cv=kf_regress_rfr,\n",
    "    scoring='neg_mean_squared_log_error',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_regress_train_X, data_regress_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnSqkYhhpNbO"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_regress_rfr.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb4MjDzXpPUm"
   },
   "outputs": [],
   "source": [
    "# feature importance（特徴量の重要度）の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': data_regress_train_X.columns,\n",
    "    'importance': hpsearch_regress_rfr.best_estimator_.feature_importances_,\n",
    "}).plot(kind='barh', x='feature', y='importance')\n",
    "plt.xlabel('importance')\n",
    "plt.title('importance of features')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQaab0M1pRfF"
   },
   "outputs": [],
   "source": [
    "# 検証データで予測する\n",
    "pred_regress_rfr_test = hpsearch_regress_rfr.best_estimator_.predict(\n",
    "    data_regress_test_X\n",
    ")\n",
    "# 検証データの予測結果を可視化する\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "pd.DataFrame({\n",
    "    'true': data_regress_test_y,\n",
    "    'predicted': pred_regress_rfr_test\n",
    "}).plot(kind='scatter', x='true', y='predicted', ax=ax, color=sns.color_palette('tab10')[0])\n",
    "maxval = max(np.max(data_regress_test_y), np.max(pred_regress_svr_test))\n",
    "ax.plot((0, maxval), (0, maxval), color='red')\n",
    "plt.xlim((0, 1.03 * maxval))\n",
    "plt.ylim((0, 1.03 * maxval))\n",
    "plt.title('prediction of `charges` (Random Forest)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeJjefNQpWyk"
   },
   "outputs": [],
   "source": [
    "# 誤差の可視化\n",
    "(pred_regress_rfr_test - data_regress_test_y).plot(kind='hist', bins=20, logy=True)\n",
    "plt.xlabel('error')\n",
    "plt.title('distribution of regresssion error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYuIJ0IrpX9g"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# 誤差の評価値\n",
    "mean_squared_log_error(data_regress_test_y, pred_regress_rfr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqqDj5rkpaSd"
   },
   "source": [
    "# モデルの Stacking\n",
    "\n",
    "* やはり回帰問題は難しい\n",
    "    * Support Vector Machine では、予測結果が３つの部分に分かれており、それぞれに bias を掛ける（ゲタを履かせる）ことが必要そう\n",
    "        * ちょうど `charges` を３つにクラスタリングした結果のように\n",
    "    * Random Forest では、真ん中のクラスタに相当するデータで誤差が大きい\n",
    "* 回帰は難しくても、分類は出来るのではないか\n",
    "    * `charges` のクラスタを予測する\n",
    "* 本当に `charges` の詳細な値を予測することが必要だったのか\n",
    "    * 例えば `charges` を予測する目的が「高額の医療費を払っている人に新しい保険商品を宣伝したい」というものであった場合\n",
    "    * 正確な医療費を予測することは必要なく、「高額の医療費を払っている人」（cluster2に相当）を予測するだけで良い\n",
    "    * **より簡単で精度の高い手法を用いて、効果的にビジネス目的（KPI）を実現する**\n",
    "* それでもやはり `charges` の値を予測することが必要だった\n",
    "    * 分類でクラスタを精度良く予測できた、とする\n",
    "    * その値を特徴量に加えることで、`charges` の値を精度良く予測することが出来ないだろうか\n",
    "    * ２段階の予測を行う（分類し、そして回帰する）\n",
    "* Ensemble と Stacking\n",
    "    * 複数のモデルを組み合わせる手法は、大きく２種類存在する\n",
    "    * Ensemble\n",
    "        * 複数のモデルの予測結果を寄せ集めて、その結果から最終的な値を予測する\n",
    "    * Stacking\n",
    "        * あるモデルの予測結果を特徴量に加え、そして次のモデルの予測を行う\n",
    "    * この場合は Stacking に相当\n",
    "* 分析方針\n",
    "    * `charges` を1次元 k-Means したものを用い、`charges` をクラスタに変換したものを目的変数とし、分類を行う\n",
    "        * 精度がわずかに良かった Random Forest を用いる\n",
    "    * その結果を特徴量に加え、`charges` を予測する回帰を行う\n",
    "        * こちらも Random Forest を用いる\n",
    "        * Support Vector Machine を用いても、偏回帰係数にかかるペナルティにより、bias に十分な値にならない\n",
    "        * Support Vecotr Machine を用いるなら、予測したクラスタ毎に分けて学習し、予測するべき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwKreQRtpoIu"
   },
   "source": [
    "## Stacking １段目：クラスタの分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cElUcXIpqOo"
   },
   "outputs": [],
   "source": [
    "# `charges`（回帰の目的変数）をクラスタ番号に変換\n",
    "data_stack_train_y = [\n",
    "    int(km_cluster_map[x][-1]) for x in km_charges.predict(\n",
    "        ss_kmeans.transform(data_regress_train_y.to_numpy().reshape(-1, 1))\n",
    "    )\n",
    "]\n",
    "data_stack_test_y = [\n",
    "    int(km_cluster_map[x][-1]) for x in km_charges.predict(\n",
    "        ss_kmeans.transform(data_regress_test_y.to_numpy().reshape(-1, 1))\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IXkqRCGpsZ-"
   },
   "outputs": [],
   "source": [
    "# クラスタに属するデータの個数の分布（学習データ）\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data_stack_train_y)).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPdB-ceapvHV"
   },
   "outputs": [],
   "source": [
    "# クラスタに属するデータの個数の分布（評価データ）\n",
    "pd.DataFrame([\n",
    "    {'key': k, 'value': v} for k, v in dict(Counter(data_stack_test_y)).items()\n",
    "]).sort_values('key').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx1TFM_9pw37"
   },
   "outputs": [],
   "source": [
    "# １段目の学習\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "hparams_stack_rfc = {\n",
    "    'n_estimators': randint(low=10, high=100),\n",
    "    'max_depth': randint(low=2, high=10),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "# Cross Validation\n",
    "# `smorker`の分類を行った時は２値分類だったが、\n",
    "# 今回は３クラスの分類（multiclass classification）であるため、\n",
    "# Cross Validation の評価値として F1 macro を用いる\n",
    "rfc_stack = RandomForestClassifier(random_state=14)\n",
    "skf_stack_rfc = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_stack_rfc = RandomizedSearchCV(\n",
    "    rfc_stack, hparams_stack_rfc, cv=skf_stack_rfc, scoring='f1_macro',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_regress_train_X, data_stack_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuSJfn6OpzDO"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_stack_rfc.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePRfZT-Lp1RR"
   },
   "outputs": [],
   "source": [
    "# 学習データで作成した最善モデルの、予測結果のF1値\n",
    "hpsearch_stack_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hkYRe0Up3RU"
   },
   "outputs": [],
   "source": [
    "# feature importance（特徴量の重要度）の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': data_regress_train_X.columns,\n",
    "    'importance': hpsearch_stack_rfc.best_estimator_.feature_importances_,\n",
    "}).plot(kind='barh', x='feature', y='importance')\n",
    "plt.xlabel('importance')\n",
    "plt.title('importance of features')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzLyzp5np5Jf"
   },
   "outputs": [],
   "source": [
    "# 検証データで予測する\n",
    "pred_stack_rfc_test = hpsearch_stack_rfc.best_estimator_.predict(\n",
    "    data_regress_test_X\n",
    ")\n",
    "# 検証データの予測結果を評価する\n",
    "pd.DataFrame(classification_report(\n",
    "    data_stack_test_y, pred_stack_rfc_test, output_dict=True,\n",
    "    target_names=['cluster0', 'cluster1', 'cluster2']\n",
    ")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1L-Et_89p8ME"
   },
   "outputs": [],
   "source": [
    "# 検証データの正解・不正解を可視化する\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(data_stack_test_y, pred_stack_rfc_test).T,\n",
    "    index=['predicted_cluster0', 'predicted_cluster1', 'predicted_cluster2'],\n",
    "    columns=['cluster0', 'cluster1', 'cluster2']\n",
    ").loc[\n",
    "    ['predicted_cluster0', 'predicted_cluster1', 'predicted_cluster2'],\n",
    "    ['cluster0', 'cluster1', 'cluster2']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei1I8SAzqAld"
   },
   "source": [
    "## Stacking ２段目：`charges` の回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN6_AHJjqAVL"
   },
   "outputs": [],
   "source": [
    "# １段目の結果を特徴量に入れ込む\n",
    "data_stack_train_X = data_regress_train_X.assign(\n",
    "    charge_cluster=hpsearch_stack_rfc.best_estimator_.predict(\n",
    "        data_regress_train_X\n",
    "    )\n",
    ")\n",
    "# クラスタ番号をダミー変数化\n",
    "data_stack_train_X = pd.concat([\n",
    "    data_stack_train_X,\n",
    "    pd.get_dummies(data_stack_train_X['charge_cluster'], prefix='charge_cluster', drop_first=True),\n",
    "], axis='columns')\n",
    "# `charge_cluster`を除去\n",
    "data_stack_train_X.drop('charge_cluster', axis='columns', inplace=True)\n",
    "# 先頭５行を表示\n",
    "data_stack_train_X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Far1khd6qF9-"
   },
   "outputs": [],
   "source": [
    "# ２段目の学習\n",
    "\n",
    "# Cross Validation で探索するHyper Parameter\n",
    "hparams_stack_rfr = {\n",
    "    'n_estimators': randint(low=10, high=200),\n",
    "    'max_depth': randint(low=2, high=10),\n",
    "}\n",
    "# Cross Validation\n",
    "rfr_stack = RandomForestRegressor(random_state=14)\n",
    "kf_stack_rfr = KFold(n_splits=5, shuffle=True, random_state=14)\n",
    "hpsearch_stack_rfr = RandomizedSearchCV(\n",
    "    rfr_stack, hparams_stack_rfr, cv=kf_stack_rfr,\n",
    "    scoring='neg_mean_squared_log_error',\n",
    "    n_iter=10, n_jobs=-1, refit=True, random_state=14\n",
    ").fit(data_stack_train_X, data_regress_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKk9T4t0qII2"
   },
   "outputs": [],
   "source": [
    "# Cross Validation の結果、最善とされた Hyper Parameter\n",
    "pd.DataFrame([\n",
    "    {'Hyper Parameter': k, 'value': v} for k, v in hpsearch_stack_rfr.best_params_.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0Qqk8psqLtH"
   },
   "outputs": [],
   "source": [
    "# feature importance（特徴量の重要度）の可視化\n",
    "pd.DataFrame({\n",
    "    'feature': data_stack_train_X.columns,\n",
    "    'importance': hpsearch_stack_rfr.best_estimator_.feature_importances_,\n",
    "}).plot(kind='barh', x='feature', y='importance')\n",
    "plt.xlabel('importance')\n",
    "plt.title('importance of features')\n",
    "plt.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hkzm5KCqODi"
   },
   "outputs": [],
   "source": [
    "# 検証データで１段目の結果の入れ込みから評価まで一気にやる\n",
    "\n",
    "# １段目の結果を特徴量に入れ込む\n",
    "data_stack_test_X = data_regress_test_X.assign(\n",
    "    charge_cluster=hpsearch_stack_rfc.best_estimator_.predict(\n",
    "        data_regress_test_X\n",
    "    )\n",
    ")\n",
    "# クラスタ番号をダミー変数化\n",
    "data_stack_test_X = pd.concat([\n",
    "    data_stack_test_X,\n",
    "    pd.get_dummies(data_stack_test_X['charge_cluster'], prefix='charge_cluster', drop_first=True),\n",
    "], axis='columns')\n",
    "# `charge_cluster`を除去\n",
    "data_stack_test_X.drop('charge_cluster', axis='columns', inplace=True)\n",
    "# `charges`の予測\n",
    "pred_stack_rfr_test = hpsearch_stack_rfr.best_estimator_.predict(\n",
    "    data_stack_test_X\n",
    ")\n",
    "# 検証データの予測結果を可視化する\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "pd.DataFrame({\n",
    "    'true': data_regress_test_y,\n",
    "    'predicted': pred_stack_rfr_test\n",
    "}).plot(kind='scatter', x='true', y='predicted', ax=ax, color=sns.color_palette('tab10')[0])\n",
    "maxval = max(np.max(data_regress_test_y), np.max(pred_stack_rfr_test))\n",
    "ax.plot((0, maxval), (0, maxval), color='red')\n",
    "plt.xlim((0, 1.03 * maxval))\n",
    "plt.ylim((0, 1.03 * maxval))\n",
    "plt.title('prediction of `charges` (Stacking)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2v9nBO2qRDL"
   },
   "outputs": [],
   "source": [
    "# 誤差の可視化\n",
    "(pred_stack_rfr_test - data_regress_test_y).plot(kind='hist', bins=20, logy=True)\n",
    "plt.xlabel('error')\n",
    "plt.title('distribution of regresssion error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2ognvQEqUyN"
   },
   "outputs": [],
   "source": [
    "# 誤差の評価値\n",
    "mean_squared_log_error(data_regress_test_y, pred_stack_rfr_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgSXMcNBqbuaQBl5kxytFy",
   "collapsed_sections": [],
   "name": "material_day03_表示用.ipynb",
   "provenance": [
    {
     "file_id": "13BEaS2ELlNe7C1lPLDb7mPaIwtSmIP2m",
     "timestamp": 1627885500370
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
